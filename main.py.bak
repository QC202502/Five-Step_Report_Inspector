# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import json # 导入json库
import time
import os
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import sys
try:
    from chromedriver_py import binary_path  # 导入chromedriver-py提供的二进制路径
    print(f"ChromeDriver binary path: {binary_path}")
    chrome_driver_available = True
except ImportError:
    print("chromedriver_py未安装，请运行: python -m pip install chromedriver-py")
    chrome_driver_available = False

# 导入Claude分析器
try:
    from claude_analyzer import ClaudeAnalyzer
    claude_analyzer = ClaudeAnalyzer()
    claude_available = True
    print("Claude分析器初始化成功")
except ImportError:
    print("未找到Claude分析器模块，将使用传统分析方法")
    claude_available = False
except Exception as e:
    print(f"初始化Claude分析器时出错: {str(e)}")
    claude_available = False

# 黄燕铭五步分析法的关键词和解释 (更加细化)
FIVE_STEP_KEYWORDS = {
    "信息": {
        "keywords": ["信息", "数据", "公告", "报告", "研究", "调研", "统计", "指标", "观察", "监测", 
                   "显示", "表明", "证实", "发现", "发布", "披露", "公布", "新闻", "消息"],
        "description": "收集和整理相关信息，包括公司公告、行业数据、政策变化等"
    },
    "逻辑": {
        "keywords": ["逻辑", "分析", "推理", "判断", "思考", "推测", "认为", "观点", "看法", "角度", 
                   "预计", "预期", "预测", "估计", "假设", "假定", "如果", "那么", "因为", "所以", 
                   "由于", "导致", "造成", "引起", "影响", "关系", "相关", "关联", "对比", "比较"],
        "description": "基于信息进行分析推理，形成对市场或个股的基本判断"
    },
    "超预期": {
        "keywords": ["超预期", "惊喜", "意外", "出乎意料", "超出预期", "好于预期", "超过预期", "超过市场预期", 
                   "超过一致预期", "出人意料", "出乎意料", "出乎预料", "超越预期", "大超预期", "远超预期", 
                   "明显好于", "显著好于", "大幅好于", "远好于", "大幅超过"],
        "description": "寻找市场共识之外的信息点，发现被低估或高估的因素"
    },
    "催化剂": {
        "keywords": ["催化剂", "驱动", "事件", "因素", "推动", "促进", "加速", "刺激", "激发", "引发", 
                   "触发", "带动", "助推", "利好", "政策", "规划", "计划", "方案", "措施", "行动", 
                   "举措", "决定", "改革", "创新", "突破", "转折", "拐点", "时点", "节点", "窗口期"],
        "description": "找出能够促使价格变动的关键事件或因素"
    },
    "结论": {
        "keywords": ["结论", "建议", "评级", "目标价", "总结", "概括", "归纳", "综上", "综合", "总的来说", 
                   "总体来看", "整体而言", "因此", "所以", "故此", "由此", "最终", "最后", "最重要", 
                   "关键", "核心", "重点", "买入", "增持", "推荐", "看好", "看涨", "看跌", "减持", "卖出"],
        "description": "给出明确的投资建议，包括评级、目标价等"
    }
}

def get_report_detail(url):
    """
    获取研报详情内容
    """
    print(f"正在获取研报详情: {url}")
    
    try:
        # 使用requests获取研报详情页
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://data.eastmoney.com/'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # 设置正确的编码
        if response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 尝试提取研报内容
        content = ""
        
        # 检查特定的zw_industry页面结构
        # 1. 尝试获取研报标题和概要
        title_div = soup.find('div', class_='report-header')
        title = title_div.find('h1').get_text(strip=True) if title_div and title_div.find('h1') else ""
        
        # 2. 尝试获取研报正文
        content_div = soup.find('div', class_='report-content')
        if content_div:
            # 移除不需要的元素
            [s.extract() for s in content_div.select('style, script')]
            content = content_div.get_text(strip=True)
        
        # 如果上述方法失败，尝试其他可能的内容容器
        if not content:
            # 尝试查找研报正文内容 (可能的其他标记)
            content_div = soup.find('div', class_='newsContent')
            if content_div:
                content = content_div.get_text(strip=True)
            else:
                # 尝试其他可能的内容容器
                content_div = soup.find('div', id='ContentBody')
                if content_div:
                    content = content_div.get_text(strip=True)
                else:
                    # 尝试查找所有段落
                    paragraphs = soup.find_all('p')
                    if paragraphs:
                        content = ' '.join([p.get_text(strip=True) for p in paragraphs])
        
        # 如果内容太长，截取前5000个字符(增加字符限制以获取更多内容)
        if len(content) > 5000:
            content = content[:5000] + "..."
            
        if content:
            print(f"成功获取研报内容，长度: {len(content)} 字符")
            # 保存前100个字符用于调试
            preview = content[:100].replace('\n', ' ')
            print(f"内容预览: {preview}...")
        else:
            print("未找到研报内容")
            
        return content
    except Exception as e:
        print(f"获取研报详情时出错: {e}")
        return ""

def get_page_with_requests(url):
    """
    使用requests库获取页面内容的备选方法
    """
    print(f"正在使用requests库获取页面: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Referer': 'https://data.eastmoney.com/'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()  # 如果状态码不是200，抛出异常
        
        # 设置正确的编码
        if response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding
        
        page_source = response.text
        print(f"获取到页面内容，长度: {len(page_source)} 字符")
        
        # 保存页面源码以便调试
        with open("page_source_requests.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        print("已保存页面源码到 page_source_requests.html 文件")
        
        return page_source
    except Exception as e:
        print(f"使用requests获取页面时出错: {e}")
        import traceback
        traceback.print_exc()
        return None

def parse_reports_from_page(page_source):
    """
    从页面内容解析研究报告
    """
    print("正在解析页面内容...")
    
    soup = BeautifulSoup(page_source, 'html.parser')
    
    # 尝试直接查找表格行
    print("尝试查找研报列表...")
    
    reports_list = []
    
    # 查找所有研报链接，格式为 /report/zw_industry.jshtml?infocode=XX
    report_links = soup.find_all('a', href=lambda href: href and 'zw_industry.jshtml' in href)
    print(f"找到 {len(report_links)} 个研报链接")
    
    if report_links:
        for link in report_links:
            try:
                # 获取研报标题
                title = link.get_text(strip=True)
                
                # 获取研报链接
                href = link.get('href')
                if href.startswith('//'):
                    full_link = f"https:{href}"
                elif href.startswith('/'):
                    full_link = f"https://data.eastmoney.com{href}"
                else:
                    full_link = href
                
                # 查找所在行
                row = link.find_parent('tr')
                if not row:
                    continue
                
                # 获取行业信息
                industry_cell = row.find('td').find('a')
                industry = industry_cell.get_text(strip=True) if industry_cell else "未知行业"
                
                # 获取评级信息
                cells = row.find_all('td')
                
                rating = ""
                org = ""
                date = ""
                
                if len(cells) >= 6:  # 确保有足够的单元格
                    rating_cell = cells[5] if len(cells) > 5 else None
                    rating = rating_cell.get_text(strip=True) if rating_cell else ""
                    
                    org_cell = cells[7] if len(cells) > 7 else None
                    org_link = org_cell.find('a') if org_cell else None
                    org = org_link.get_text(strip=True) if org_link else ""
                    
                    date_cell = cells[9] if len(cells) > 9 else None
                    date = date_cell.get_text(strip=True) if date_cell else ""
                
                # 构建报告摘要
                abstract = f"行业: {industry}, 评级: {rating}, 机构: {org}, 日期: {date}"
                
                reports_list.append({
                    "title": title,
                    "link": full_link,
                    "abstract": abstract,
                    "industry": industry,
                    "rating": rating,
                    "org": org,
                    "date": date
                })
                print(f"解析到研报: {title}")
            except Exception as e:
                print(f"解析研报链接时出错: {e}")
                continue
    
    # 如果未找到任何研报，使用备选逻辑
    if not reports_list:
        print("未找到任何研报，尝试备选解析逻辑...")
        # 寻找行业研报表格
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            if len(rows) > 1:  # 确保表格有内容（标题行+数据行）
                for row in rows[1:]:  # 跳过标题行
                    try:
                        cells = row.find_all('td')
                        if len(cells) < 5:
                            continue
                            
                        # 尝试从单元格中提取研报数据
                        industry_cell = cells[1].find('a') if len(cells) > 1 else None
                        industry = industry_cell.get_text(strip=True) if industry_cell else "未知行业"
                        
                        report_cell = cells[4].find('a') if len(cells) > 4 else None
                        if not report_cell or not report_cell.get('href') or 'zw_industry.jshtml' not in report_cell.get('href'):
                            continue
                            
                        title = report_cell.get_text(strip=True)
                        href = report_cell.get('href')
                        
                        if href.startswith('//'):
                            full_link = f"https:{href}"
                        elif href.startswith('/'):
                            full_link = f"https://data.eastmoney.com{href}"
                        else:
                            full_link = href
                        
                        rating_cell = cells[5] if len(cells) > 5 else None
                        rating = rating_cell.get_text(strip=True) if rating_cell else ""
                        
                        org_cell = cells[7] if len(cells) > 7 else None
                        org_link = org_cell.find('a') if org_cell else None
                        org = org_link.get_text(strip=True) if org_link else ""
                        
                        date_cell = cells[9] if len(cells) > 9 else None
                        date = date_cell.get_text(strip=True) if date_cell else ""
                        
                        # 构建报告摘要
                        abstract = f"行业: {industry}, 评级: {rating}, 机构: {org}, 日期: {date}"
                        
                        reports_list.append({
                            "title": title,
                            "link": full_link,
                            "abstract": abstract,
                            "industry": industry,
                            "rating": rating,
                            "org": org,
                            "date": date
                        })
                        print(f"解析到研报: {title}")
                    except Exception as e:
                        print(f"解析行时出错: {e}")
                        continue
    
    # 如果仍然没有找到研报，进行最后的备选尝试
    if not reports_list:
        print("未能使用结构化方式找到研报，尝试提取所有包含'zw_industry'的链接...")
        for a in soup.find_all('a'):
            href = a.get('href', '')
            text = a.get_text(strip=True)
            if 'zw_industry.jshtml' in href and text:
                try:
                    if href.startswith('//'):
                        full_link = f"https:{href}"
                    elif href.startswith('/'):
                        full_link = f"https://data.eastmoney.com{href}"
                    else:
                        full_link = href
                        
                    reports_list.append({
                        "title": text,
                        "link": full_link,
                        "abstract": "行业研报",
                        "industry": "未知",
                        "rating": "",
                        "org": "",
                        "date": ""
                    })
                    print(f"解析到研报: {text}")
                except Exception as e:
                    print(f"提取研报链接时出错: {e}")
    
    print(f"共解析出 {len(reports_list)} 条研报数据")
    return reports_list

def scrape_research_reports(url):
    """
    使用 Selenium 爬取东方财富网的研究报告摘要。
    如果Selenium方法失败，将使用requests备选方法。
    """
    print(f"正在使用 Selenium 爬取页面：{url}")

    driver = None # 初始化 driver 为 None
    try:
        if not chrome_driver_available:
            print("ChromeDriver不可用，将使用requests备选方法")
            page_source = get_page_with_requests(url)
            if page_source:
                return parse_reports_from_page(page_source)
            return []
        
        # 设置Chrome选项，确保浏览器窗口可见
        chrome_options = Options()
        chrome_options.add_argument("--start-maximized")  # 最大化窗口
        chrome_options.add_argument("--disable-extensions")  # 禁用扩展
        chrome_options.add_argument("--disable-gpu")  # 禁用GPU加速
        chrome_options.add_argument("--no-sandbox")  # 禁用沙箱模式
        chrome_options.add_argument("--disable-dev-shm-usage")  # 禁用/dev/shm使用
        chrome_options.headless = False  # 明确设置为非无头模式
        
        print("正在初始化Chrome浏览器...")
        
        # 创建Service对象，使用chromedriver-py提供的ChromeDriver路径
        service = Service(executable_path=binary_path)
        
        # 设置超时
        service.start()
        
        # 初始化 Chrome 浏览器驱动，使用service对象
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        print("Chrome浏览器已初始化，正在访问URL...")
        
        # 设置页面加载超时
        driver.set_page_load_timeout(30)
        
        # 访问URL
        try:
            driver.get(url)
            print(f"已访问URL: {url}")
            
            # 等待页面加载，这里简单等待几秒，实际应用中可以等待某个特定元素出现
            print("等待页面加载(5秒)...")
            time.sleep(5) # 暂停5秒，等待js加载数据，可能需要根据实际情况调整
            
            # 获取页面渲染后的源代码
            page_source = driver.page_source
            print(f"已获取页面源代码，长度: {len(page_source)} 字符")
            
            # 保存页面源码以便调试
            with open("page_source_selenium.html", "w", encoding="utf-8") as f:
                f.write(page_source)
            print("已保存页面源码到 page_source_selenium.html 文件")
            
            return parse_reports_from_page(page_source)
            
        except Exception as e:
            print(f"Selenium访问URL时出错: {e}")
            print("将尝试使用requests备选方法...")
            page_source = get_page_with_requests(url)
            if page_source:
                return parse_reports_from_page(page_source)
            return []

    except Exception as e:
        print(f"爬取过程中发生错误: {e}")
        import traceback
        traceback.print_exc()
        
        # 尝试使用备选方法
        print("将尝试使用requests备选方法...")
        page_source = get_page_with_requests(url)
        if page_source:
            return parse_reports_from_page(page_source)
        return [] # 发生错误时返回空列表
    finally:
        if driver:
            print("关闭浏览器...")
            try:
                driver.quit() # 确保关闭浏览器
            except Exception as e:
                print(f"关闭浏览器时遇到错误: {e}")
    return []

def analyze_with_five_steps(summary, content="", industry=None, use_claude=True):
    """
    使用黄燕铭五步分析法对报告摘要和内容进行分析。
    默认使用Claude进行高级语义分析，提供更准确和详细的结果。
    
    黄燕铭五步分析法:
    1. 信息：收集和整理相关信息，包括公司公告、行业数据、政策变化等
    2. 逻辑：基于信息进行分析推理，形成对市场或个股的基本判断
    3. 超预期：寻找市场共识之外的信息点，发现被低估或高估的因素
    4. 催化剂：找出能够促使价格变动的关键事件或因素
    5. 结论：给出明确的投资建议，包括评级、目标价等
    
    Parameters:
    -----------
    summary : str
        报告摘要
    content : str, optional
        报告内容
    industry : str, optional
        行业分类
    use_claude : bool, optional
        是否使用Claude进行分析，默认为True
        
    Returns:
    --------
    dict
        分析结果字典
    """
    # 尝试使用Claude进行分析
    if claude_available:
        try:
            print("使用Claude进行高级语义分析...")
            # 提取标题
            title = summary.split('\n')[0] if '\n' in summary else summary[:100]
            # 使用Claude分析器进行分析
            result = claude_analyzer.analyze_with_five_steps(title, content, industry)
            print("Claude分析完成")
            return result["analysis"]
        except Exception as e:
            print(f"Claude分析失败，将使用传统方法: {str(e)}")
            # 如果Claude分析失败，回退到传统方法
    else:
        print("Claude分析器不可用，将使用传统关键词匹配方法")
    
    # 传统的关键词匹配分析方法（仅在Claude不可用时使用）
    print("使用关键词匹配进行基础分析...")
    analysis_results = {}
    
    # 合并摘要和内容进行分析
    full_text = f"{summary} {content}"
    
    for step, step_info in FIVE_STEP_KEYWORDS.items():
        keywords = step_info["keywords"]
        description = step_info["description"]
        
        # 查找匹配的关键词
        found_keywords = []
        for keyword in keywords:
            # 使用正则表达式进行更精确的匹配
            pattern = r'[^\w]{}[^\w]|^{}[^\w]|[^\w]{}$|^{}$'.format(keyword, keyword, keyword, keyword)
            if re.search(pattern, full_text, re.IGNORECASE):
                found_keywords.append(keyword)
        
        # 提取包含关键词的句子作为证据
        evidence = []
        if found_keywords:
            # 将文本分割成句子
            sentences = re.split(r'[。！？.!?]', full_text)
            for sentence in sentences:
                for keyword in found_keywords:
                    if keyword in sentence:
                        # 清理句子并添加到证据中
                        clean_sentence = sentence.strip()
                        if clean_sentence and len(clean_sentence) > 5:  # 避免太短的句子
                            evidence.append(clean_sentence)
                            break  # 每个句子只添加一次
        
        # 限制证据数量，避免太多
        if len(evidence) > 3:
            evidence = evidence[:3]
            
        # 构建分析结果
        analysis_results[step] = {
            "found": len(found_keywords) > 0,
            "keywords": found_keywords,
            "evidence": evidence,
            "description": description
        }
    
    # 计算五步法完整度分数 (0-100)
    steps_found = sum(1 for step_result in analysis_results.values() if step_result["found"])
    completeness_score = int((steps_found / 5) * 100)
    
    # 添加总结
    analysis_results["summary"] = {
        "completeness_score": completeness_score,
        "steps_found": steps_found,
        "evaluation": get_evaluation_text(completeness_score)
    }
    
    return analysis_results

def get_evaluation_text(score):
    """根据完整度分数生成评价文本"""
    if score >= 90:
        return "研报非常完整地应用了五步分析法，包含了全面的分析要素"
    elif score >= 80:
        return "研报较好地应用了五步分析法，大部分分析要素齐全"
    elif score >= 60:
        return "研报部分应用了五步分析法，关键分析要素有所欠缺"
    elif score >= 40:
        return "研报仅包含少量五步分析法要素，分析不够全面"
    else:
        return "研报几乎未应用五步分析法，分析要素严重不足"

def save_results(data, filename="research_reports.json"):
    """
    将爬取和分析结果保存到 JSON 文件。
    """
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"结果已保存到 {filename}")

# 主程序入口
if __name__ == "__main__":
    print(f"Python版本: {sys.version}")
    print(f"Selenium版本: {webdriver.__version__}")
    print(f"Requests版本: {requests.__version__}")
    
    # 东方财富网行业研报页面 URL
    report_url = "https://data.eastmoney.com/report/hyyb.html"

    # 爬取研究报告列表
    reports_data = scrape_research_reports(report_url)
    print(f"爬取到 {len(reports_data)} 条研报数据")

    # 对每份报告进行五步分析
    analyzed_reports = []
    
    # 限制处理的报告数量，避免请求过多
    max_reports = min(10, len(reports_data))
    print(f"将处理前 {max_reports} 条研报数据")
    
    for i, report in enumerate(reports_data[:max_reports]):
        print(f"\n处理第 {i+1}/{max_reports} 条研报: {report.get('title', 'N/A')}")
        
        # 获取研报详情内容
        content = get_report_detail(report.get("link", ""))
        
        # 获取行业信息
        industry = report.get("industry", "N/A")
        
        # 进行五步分析，使用Claude
        analysis = analyze_with_five_steps(
            report.get("abstract", ""),
            content,
            industry=industry
        )
        
        analyzed_reports.append({
            "title": report.get("title", "N/A"),
            "link": report.get("link", "N/A"),
            "abstract": report.get("abstract", "N/A"),
            "content_preview": content[:200] + "..." if content else "未获取到内容",
            "industry": industry,
            "rating": report.get("rating", "N/A"),
            "org": report.get("org", "N/A"),
            "date": report.get("date", "N/A"),
            "analysis": analysis,
            "analysis_method": "Claude增强"
        })

    # 保存结果
    save_results(analyzed_reports)

    print("\n分析完成！请查看 research_reports.json 文件获取详细结果。")
    print("使用Claude增强五步法分析已完成。") 